"""Statistical thinking in Python Part 2"""

"""Chapter 1 - Optimal Parameters"""
# Optimal parameters - parameter values that bring the model in closest agreement with the data

# Packages to do statistical inference:
#  - scipy.stats
#  - statsmodels
# this course - hacker stats with numpy

# Linear regression by least squares
# Want to select the slope and intercept such that the points lie as close as possible to the line
# Residual - delta from the line (below = negative, above = positive)
# Least squares - process of finding params for which the sum of the squares of the residuals is minimal
# np.polyfit() - does least squares with polynomial functions

slope, intercept = np.polyfit(total_votes,  # x arg
                              dem_share,   # y arg
                              1)  # Degree of polynomial you wish to fit
#  Returns slop and intercept of best fit line

# The importance of EDA - Anscombe's Quartet - reminder to do Graphical EDA first

"""Chapter 2 - Bootstrap confidence intervals"""

# Generating bootstrap replicates
# Resampling an array to replicate the mean
# Sampling with replacement - sample an entry from an array and retain in original array
# Repeat n times, where n is the total number of measurements
# Can use to get an ECDF - probability distribution used to describe summary statistic
# Bootstrapping - the use of resampled data to perform statistical inference
# Bootstrap sample - resampled array of the data
# Bootstrap replicate - statistic computed from a resampled array (a simulated replica)

# Resampling engine
import numpy as np
np.random.choice([1, 2, 3, 4, 5], size=5)  # size - specifies number of samples to take out of the array
# Function does not delete when samples, therefore can sample the same number multiple times

bs_sample = np.random.choice(michaelson_speed_of_light, size=100)
np.mean(bs_sample)
np.median(bs_sample)
np.std(bs_sample)

# Bootstrap confidence intervals
# Writing a function to generate a bootstrap replicate


def bootstrap_replicate_1d(data, func):
    """Generate bootstrap replicate of 1D data."""
    bs_sample = np.random.choice(data, len(data))
    return func(bs_sample)

# Many bootstrap replicates
bs_replicates = np.empty(10000)  # Empty array to store bootstrap replicates
for i in range(10000):  # Create a for loop to create the bootstrap replicate and store in the array
    bs_replicates[i] = bootstrap_replicate_1d(michaelson_speed_of_light, np.mean)

# Plotting a histogram of bootstrap replicates
_ = plt.hist(bs_replicates, bins=30, normed=True)
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()

# Confidence interval of a statistic
# If we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval

# Bootstrap confidence interval
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Theoretically - the value of the mean will always be normally distributed
# Standard error of the mean - std of this distribution

# Pairs bootstrap

# Non-parametric inference
# Make no assumptions about the model or probability distribution underlying the data
# Linear regression - parametric estimate

# Paris bootstrap for linear regression - method that makes the least assumptions
# Example - US election data
# Can't resample individual data as each county has two variables associated with it
# Instead - sample data in pairs
# Compute slope and intercept from resampled data
# Each slope and intercept is a bootstrap replicate
# Compute confidence intervals from percentiles of bootstrap replicates

# Generating pairs from a bootstrap sample
# Sample the indices of the datapoints
np.arange(7)  # Generates indices, a range of sequential integers - e.g. an array 0 to 6

inds = np.arange(len(total_votes))
bs_inds = np.random.choice(inds, len(inds))  # Sample indices with replacement
# Bootstrap generated by slicing out respective values from original data arrays
bs_total_votes = total_votes[bs_inds]
bs_dem_share = dem_share[bs_inds]

# Computing a pairs bootstrap replicate

bs_slope, bs_intercept = np.polyfit(bs_total_votes,
                                    bs_dem_share, 1)

# Can plot the lines from the bootstrap regressions to show how the regression may change if the data were collected again


""" Chapter 3 - Formulating and simulating a hypothesis"""
# Formulating and simulating a hypothesis

# Hypothesis testing
#  - Assessment of how reasonable the observed data are assuming a hypothesis is true

# Null hypothesis - hypothesis you are assessing

# Simulating the hypothesis

# E.g. for voting data
# Put the data together, randomly scramble the ordering of the counties
# Then relable the first 67 to be "Pennsylvania" and remainder to be "Ohio"

# Permutation
# Random ordering of entries in an array

# Generating a permutation sample
import numpy as np
dem_share_both = np.concatenate((dem_share_PA, dem_share_OH))  # Takes a tuple of the arrays
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)]  # Permutation samples
perm_sample_OH = dem_share_perm[len(dem_share_PA):]

# Test statistics and p values

# Test statistic
# A single number that can be computed from observed data and from data you simulated under the null hypothesis
# It serves as a basis of comparison between the two
# E.g. difference in mean vote share

# Permutation replicate
# The value of a test statistic computed from a permutation sample

np.mean(perm_sample_PA) - np.mean(perm_sample_OH)  # Perm replicate
np.mean(dem_share_PA) - np.mean(dem_share_OH)  # Original data

# Can generate lots of permutation replicates and plot a histogram

# p-value
# The probability of obtaining a value of your test statistic that is at least as extreme as what was observed,
# under the assumption that the null hypothesis is true
# NOT the probability that the null hypothesis is true
# Only meaningful if null hypothesis is clearly stated, along with test statistic used to evaluate it

# Statistical significance
# Determined by the smallness of a p-value

# Null Hypothesis Significance Testing (NHST)

# Statistical significance and practical significance are different things!

# Bootstrap hypothesis tests

# Pipeline for hypothesis testing
#  - Clearly state the null hypothesis
#  - Define your test statistic
#  - Generate many sets of simulated data assuming the null hypothesis is true
#  - Compute the test statistic for each simulated data set
#  - The p-value is the fraction of your simulated data sets for which the statistic is at least as extreme as for the real data

# Example - Michaelson and Newcomb
# Michaelson - have dataset
# Newcomb - only have a calculated mean

# Null hypothesis
# The true mean speed of light in Michaelson's experiment was actually Newcomb's reported value

# Shift Michaelson's data, such that it's mean matches Newcomb's value
newcomb_value = 299860  # km/s
michelson_shifted = michaelson_speed_of_light - np.mean(michaelson_speed_of_light) + newcomb_value

# Use bootstrapping on shifted data
# Calculating the test statistic

def diff_from_newcomb(data, newcomb_value=299860):
    return np.mean(data) - newcomb_value

diff_obs = diff_from_newcomb(michelson_speed_of_light)

# Computing the p-value
bs_replicates = draw_bs_reps(michelson_shifted, diff_from_newcomb, 10000)

p_value = np.sum(bs_replicates <= diff_observed) / 10000

# One sample test - compare one set of data to a single number
# Two sample test - compare two sets of data

""" Chapter 4 - Hypothesis Test Examples"""
# A/B Testing
# Used by organisations to see if their strategy gets a better result
# Null - generally that the test statistic is impervious to the change
# Low p value - change in strategy led to change in performance

# Null hypothesis - the click through rate is not affected by the design

import numpy as np
# clickthrough_A, clickthrough_B: arr. of 1s and 0s
def diff_frac(data_A, data_B):
    frac_A = np.sum(data_A) / len(data_A)
    frac_B = np.sum(data_B) / len(data_B)
    return frac_B - frac_A

# Using observed data
diff_frac_obs = diff_frac(clickthrough_A, clickthrough_B)

# Create replicates
perm_replicates = np.empty(10000)
for i in range(10000):
    perm_replicates[i] = permutation_replicate(clickthrough_A, clickthrough_B, diff_frac)

p_value = np.sum(perm_replicates >= diff_frac_obs) / 10000

# Test of correlation
# Null hypothesis - there is no correlation between the two variables
# Simulate data assuming null hypothesis is true
# Use Pearson correlation as test statistic
# Compute p-value as fraction of replicates that have p at least as large as observed

""" Chapter 5 - A Case Study"""
# Beak length
# Beak depth

# Investigation of G. scandens beak depth
# EDA of beak depths in 1975 and 2012
# Parameter estimates of mean beak depth
# Hypothesis test - did the beaks get deeper?

# Variation in beak shapes

# Calculation of heritability - tendency for parental traits to be inherited by offspring